参数：
MLP：1500*256*3
lr = 1e-3  # 学习率
batch_size = 500  # batch
epoch = 10000  # 训练次数
val_time = 500  # 每训练多少次验证一次
weight = 0.175
删除0.9的全O


基本信息统计：
训练集大小:481408 验证集大小：112189 测试集大小：223834
-------------train-------------
0/10000
loss=121283.390625
hit=2010 p_sum=104947 r_sum=4532
precision=0.019152524607659104 recall=0.4435127978817299
f1-measure=0.03671937083824295

500/10000
loss=66393.140625
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

1000/10000
loss=65709.390625
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

1500/10000
loss=65549.9453125
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

2000/10000
loss=65464.23046875
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

2500/10000
loss=65445.87109375
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

3000/10000
loss=65459.265625
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

3500/10000
loss=65608.578125
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

4000/10000
loss=66670.9609375
hit=189 p_sum=288 r_sum=4532
precision=0.65625 recall=0.04170344218887908
f1-measure=0.0784232365145228

4500/10000
loss=67586.828125
hit=645 p_sum=1422 r_sum=4532
precision=0.45358649789029537 recall=0.14232127096204766
f1-measure=0.21666106818945244

5000/10000
loss=68530.9296875
hit=794 p_sum=2797 r_sum=4532
precision=0.283875580979621 recall=0.17519858781994704
f1-measure=0.21667348887979257

5500/10000
loss=70730.53125
hit=991 p_sum=3430 r_sum=4532
precision=0.28892128279883383 recall=0.21866725507502208
f1-measure=0.24893242903793017

6000/10000
loss=71161.015625
hit=1353 p_sum=4515 r_sum=4532
precision=0.29966777408637874 recall=0.29854368932038833
f1-measure=0.29910467558306625

6500/10000
loss=70149.6015625
hit=1855 p_sum=5306 r_sum=4532
precision=0.3496042216358839 recall=0.4093115622241836
f1-measure=0.377109168530189

7000/10000
loss=71103.890625
hit=2516 p_sum=7462 r_sum=4532
precision=0.33717502010184935 recall=0.5551632833186231
f1-measure=0.41954310488577623

7500/10000
loss=69174.1328125
hit=2615 p_sum=6631 r_sum=4532
precision=0.3943598250640929 recall=0.5770079435127978
f1-measure=0.4685120487324196

8000/10000
loss=70865.3125
hit=2955 p_sum=8585 r_sum=4532
precision=0.3442050087361677 recall=0.6520300088261254
f1-measure=0.45056034154151103

8500/10000
loss=69034.2265625
hit=2951 p_sum=7866 r_sum=4532
precision=0.3751589117721841 recall=0.6511473962930273
f1-measure=0.47604452331021124

9000/10000
loss=67048.453125
hit=2828 p_sum=6193 r_sum=4532
precision=0.4566445987405135 recall=0.6240070609002648
f1-measure=0.5273659673659674

9500/10000
loss=68246.25
hit=3013 p_sum=7483 r_sum=4532
precision=0.40264599759454767 recall=0.6648278905560459
f1-measure=0.5015397419891802

10000/10000
loss=67800.6015625
hit=3045 p_sum=7294 r_sum=4532
precision=0.41746641074856045 recall=0.6718887908208296
f1-measure=0.5149670218163368


-------------test-------------

loss=135247.921875
hit=5596 p_sum=13865 r_sum=8606
precision=0.40360620266859 recall=0.6502440158029282
f1-measure=0.49806417159894983
