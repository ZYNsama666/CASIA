参数：
MLP：1500*256*3
lr = 1e-3  # 学习率
batch_size = 500  # batch
epoch = 10000  # 训练次数
val_time = 500  # 每训练多少次验证一次
weight = 0.075
删除0.9的全O

基本信息统计：
训练集大小:481408 验证集大小：112189 测试集大小：223834
-------------train-------------
0/10000
loss=122645.390625
hit=2489 p_sum=109869 r_sum=4532
precision=0.02265425188178649 recall=0.5492056487202118
f1-measure=0.043513605650300255

500/10000
loss=94431.1953125
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

1000/10000
loss=90162.9765625
hit=1018 p_sum=8916 r_sum=4532
precision=0.11417676087931808 recall=0.22462488967343336
f1-measure=0.15139797739440808

1500/10000
loss=90094.375
hit=2303 p_sum=18402 r_sum=4532
precision=0.12514944027823063 recall=0.5081641659311562
f1-measure=0.20083718496555336

2000/10000
loss=87843.78125
hit=3107 p_sum=20295 r_sum=4532
precision=0.1530918945553092 recall=0.6855692850838482
f1-measure=0.25029202078382407

2500/10000
loss=81328.8984375
hit=3169 p_sum=17147 r_sum=4532
precision=0.1848136700297428 recall=0.6992497793468667
f1-measure=0.2923566585174593

3000/10000
loss=79784.25
hit=3389 p_sum=17653 r_sum=4532
precision=0.1919787005041636 recall=0.747793468667255
f1-measure=0.3055217489294568

3500/10000
loss=79455.2578125
hit=3525 p_sum=18344 r_sum=4532
precision=0.19216092455298736 recall=0.777802294792586
f1-measure=0.3081832488197237

4000/10000
loss=81787.734375
hit=3748 p_sum=21320 r_sum=4532
precision=0.17579737335834897 recall=0.8270079435127978
f1-measure=0.2899582237351075

4500/10000
loss=80412.4375
hit=3779 p_sum=20704 r_sum=4532
precision=0.18252511591962906 recall=0.8338481906443072
f1-measure=0.2994927880805199

5000/10000
loss=77371.0625
hit=3705 p_sum=18153 r_sum=4532
precision=0.2040984961163444 recall=0.8175198587819947
f1-measure=0.32664756446991405

5500/10000
loss=77031.7578125
hit=3744 p_sum=17853 r_sum=4532
precision=0.20971265333557385 recall=0.8261253309796999
f1-measure=0.33450971632789817

6000/10000
loss=79076.734375
hit=3860 p_sum=20553 r_sum=4532
precision=0.18780713277866978 recall=0.851721094439541
f1-measure=0.30775363763205105

6500/10000
loss=75351.5
hit=3716 p_sum=16177 r_sum=4532
precision=0.22970884589231624 recall=0.8199470432480142
f1-measure=0.35887778260659614

7000/10000
loss=76807.5234375
hit=3839 p_sum=18659 r_sum=4532
precision=0.20574521678546545 recall=0.8470873786407767
f1-measure=0.33107671079298

7500/10000
loss=77816.2734375
hit=3889 p_sum=19823 r_sum=4532
precision=0.19618624829743228 recall=0.8581200353045013
f1-measure=0.3193594744405666

8000/10000
loss=74960.7265625
hit=3795 p_sum=16284 r_sum=4532
precision=0.2330508474576271 recall=0.837378640776699
f1-measure=0.3646233666410453

8500/10000
loss=73736.34375
hit=3766 p_sum=15455 r_sum=4532
precision=0.24367518602394048 recall=0.8309796999117387
f1-measure=0.376844949216991

9000/10000
loss=75214.296875
hit=3860 p_sum=17290 r_sum=4532
precision=0.22325043377674958 recall=0.851721094439541
f1-measure=0.3537714233342498

9500/10000
loss=72350.34375
hit=3704 p_sum=14141 r_sum=4532
precision=0.2619333851919949 recall=0.8172992056487202
f1-measure=0.3967225405665935

10000/10000
loss=73318.6640625
hit=3805 p_sum=15367 r_sum=4532
precision=0.2476085117459491 recall=0.839585172109444
f1-measure=0.382431277953666


-------------test-------------

loss=146481.6875
hit=7187 p_sum=30079 r_sum=8606
precision=0.23893746467635227 recall=0.8351150360213805
f1-measure=0.3715652061522554
