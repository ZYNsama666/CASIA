参数：
MLP：1500*256*3
lr = 1e-3  # 学习率
batch_size = 500  # batch
epoch = 10000  # 训练次数
val_time = 500  # 每训练多少次验证一次
weight = 0.15
删除0.9的全O



基本信息统计：
训练集大小:481408 验证集大小：112189 测试集大小：223834
-------------train-------------
0/10000
loss=121091.1171875
hit=1951 p_sum=109514 r_sum=4532
precision=0.017815073871833738 recall=0.4304942630185349
f1-measure=0.03421426441962015

500/10000
loss=67681.03125
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

1000/10000
loss=66769.9921875
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

1500/10000
loss=66801.46875
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

2000/10000
loss=69558.0859375
hit=261 p_sum=383 r_sum=4532
precision=0.6814621409921671 recall=0.057590467784642545
f1-measure=0.10620549338758903

2500/10000
loss=71240.703125
hit=736 p_sum=2296 r_sum=4532
precision=0.3205574912891986 recall=0.1624007060900265
f1-measure=0.2155828939660223

3000/10000
loss=72566.7734375
hit=969 p_sum=3339 r_sum=4532
precision=0.29020664869721474 recall=0.21381288614298324
f1-measure=0.2462203023758099

3500/10000
loss=73546.75
hit=1622 p_sum=5511 r_sum=4532
precision=0.29432045000907275 recall=0.35789938217122685
f1-measure=0.3230110524743602

4000/10000
loss=71999.6875
hit=2234 p_sum=6689 r_sum=4532
precision=0.33398116310360293 recall=0.4929390997352162
f1-measure=0.39818198021566703

4500/10000
loss=73879.75
hit=2946 p_sum=11569 r_sum=4532
precision=0.254646036822543 recall=0.6500441306266549
f1-measure=0.3659400037264766

5000/10000
loss=69556.5234375
hit=2694 p_sum=7289 r_sum=4532
precision=0.36959802442035944 recall=0.5944395410414828
f1-measure=0.4557990017764994

5500/10000
loss=70354.046875
hit=2967 p_sum=8559 r_sum=4532
precision=0.3466526463371889 recall=0.6546778464254193
f1-measure=0.45328851882973037

6000/10000
loss=73008.5625
hit=3343 p_sum=12782 r_sum=4532
precision=0.26153966515412297 recall=0.7376434245366285
f1-measure=0.38616148781333026

6500/10000
loss=67996.4296875
hit=2957 p_sum=7150 r_sum=4532
precision=0.4135664335664336 recall=0.6524713150926743
f1-measure=0.5062489299777435

7000/10000
loss=71956.40625
hit=3441 p_sum=12753 r_sum=4532
precision=0.26981886614914136 recall=0.7592674315975286
f1-measure=0.3981486838299103

7500/10000
loss=69626.109375
hit=3252 p_sum=9228 r_sum=4532
precision=0.35240572171651496 recall=0.7175639894086496
f1-measure=0.4726744186046512

8000/10000
loss=68910.875
hit=3248 p_sum=8993 r_sum=4532
precision=0.3611697987323474 recall=0.7166813768755517
f1-measure=0.4802957486136784

8500/10000
loss=69569.0234375
hit=3318 p_sum=9559 r_sum=4532
precision=0.34710743801652894 recall=0.7321270962047661
f1-measure=0.47093889716840537

9000/10000
loss=68898.90625
hit=3304 p_sum=9174 r_sum=4532
precision=0.3601482450403314 recall=0.7290379523389232
f1-measure=0.4821246169560777

9500/10000
loss=67876.40625
hit=3239 p_sum=8265 r_sum=4532
precision=0.39189352692075013 recall=0.7146954986760812
f1-measure=0.5062123935297335

10000/10000
loss=67975.4296875
hit=3279 p_sum=8433 r_sum=4532
precision=0.3888295980078264 recall=0.7235216240070609
f1-measure=0.5058233706131894


-------------test-------------

loss=136416.125
hit=6143 p_sum=17031 r_sum=8606
precision=0.36069520286536316 recall=0.7138043225656518
f1-measure=0.4792292389905215
