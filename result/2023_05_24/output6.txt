参数：
MLP：1500*256*3
lr = 1e-3  # 学习率
batch_size = 500  # batch
epoch = 10000  # 训练次数
val_time = 500  # 每训练多少次验证一次
weight = 0.1
删除0.9的全O

基本信息统计：
训练集大小:481408 验证集大小：112189 测试集大小：223834
-------------train-------------
0/10000
loss=123030.0
hit=2208 p_sum=109869 r_sum=4532
precision=0.020096660568495208 recall=0.4872021182700794
f1-measure=0.03860106117953514

500/10000
loss=78961.1875
hit=0 p_sum=0 r_sum=4532
precision=0 recall=0.0
f1-measure=0

1000/10000
loss=83107.6640625
hit=573 p_sum=3310 r_sum=4532
precision=0.17311178247734138 recall=0.1264342453662842
f1-measure=0.14613618974751338

1500/10000
loss=82758.0546875
hit=1533 p_sum=9758 r_sum=4532
precision=0.15710186513629842 recall=0.338261253309797
f1-measure=0.21455563331000702

2000/10000
loss=80861.6953125
hit=2465 p_sum=13043 r_sum=4532
precision=0.18899026297630914 recall=0.543909973521624
f1-measure=0.2805120910384068

2500/10000
loss=76535.15625
hit=2765 p_sum=11382 r_sum=4532
precision=0.24292742927429276 recall=0.6101059135039718
f1-measure=0.347492773658414

3000/10000
loss=77911.7734375
hit=3229 p_sum=15052 r_sum=4532
precision=0.21452298697847463 recall=0.7124889673433362
f1-measure=0.3297589869281046

3500/10000
loss=78321.703125
hit=3463 p_sum=17157 r_sum=4532
precision=0.20184181383691788 recall=0.7641218005295676
f1-measure=0.31933238046936235

4000/10000
loss=77024.921875
hit=3483 p_sum=15711 r_sum=4532
precision=0.22169180828718732 recall=0.7685348631950574
f1-measure=0.3441189547003902

4500/10000
loss=76030.4375
hit=3523 p_sum=15298 r_sum=4532
precision=0.2302915413779579 recall=0.7773609885260371
f1-measure=0.3553202218860313

5000/10000
loss=75716.5859375
hit=3600 p_sum=15719 r_sum=4532
precision=0.22902220243018004 recall=0.794351279788173
f1-measure=0.3555379981235494

5500/10000
loss=73475.1171875
hit=3553 p_sum=14120 r_sum=4532
precision=0.251628895184136 recall=0.7839805825242718
f1-measure=0.3809779112159554

6000/10000
loss=75876.0390625
hit=3713 p_sum=16344 r_sum=4532
precision=0.2271781693587861 recall=0.8192850838481907
f1-measure=0.35571948649166507

6500/10000
loss=74670.5625
hit=3703 p_sum=15397 r_sum=4532
precision=0.2405013963759174 recall=0.8170785525154457
f1-measure=0.3716192483315771

7000/10000
loss=70803.1484375
hit=3407 p_sum=10553 r_sum=4532
precision=0.3228465839097887 recall=0.7517652250661959
f1-measure=0.45170699370235334

7500/10000
loss=73617.65625
hit=3718 p_sum=15188 r_sum=4532
precision=0.24479852515143535 recall=0.8203883495145631
f1-measure=0.37707910750507095

8000/10000
loss=73280.2734375
hit=3736 p_sum=15026 r_sum=4532
precision=0.24863569812325303 recall=0.824360105913504
f1-measure=0.38204315369669706

8500/10000
loss=74030.3125
hit=3765 p_sum=15662 r_sum=4532
precision=0.24039075469288723 recall=0.8307590467784642
f1-measure=0.3728830345647222

9000/10000
loss=74018.2890625
hit=3772 p_sum=15718 r_sum=4532
precision=0.2399796411757221 recall=0.8323036187113857
f1-measure=0.37254320987654327

9500/10000
loss=72842.171875
hit=3747 p_sum=14820 r_sum=4532
precision=0.252834008097166 recall=0.8267872903795234
f1-measure=0.38724679619677554

10000/10000
loss=73505.015625
hit=3791 p_sum=15367 r_sum=4532
precision=0.24669746860154879 recall=0.836496028243601
f1-measure=0.38102417206894823


-------------test-------------

loss=147405.671875
hit=7216 p_sum=31276 r_sum=8606
precision=0.2307200409259496 recall=0.8384847780618173
f1-measure=0.3618675091519984
